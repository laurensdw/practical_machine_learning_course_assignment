object <- enet(data.matrix(training), data.matrix(training$CompressiveStrength), lambda=1)
plot(object, xvar="penalty", use.color = T)
training = filter(training, -CompressiveStrength)
training = select(training, -CompressiveStrength)
set.seed(233)
object <- enet(data.matrix(training), data.matrix(training$CompressiveStrength), lambda=1)
plot(object, xvar="penalty", use.color = T)
training
head(training)
## Vraag 3
set.seed(3523)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
training_train = select(training, -CompressiveStrength)
set.seed(233)
object <- enet(data.matrix(training_train), data.matrix(training$CompressiveStrength), lambda=1)
plot(object, xvar="penalty", use.color = T)
library(data.table)
mydat <- fread('https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv')
head(mydat)
library(lubridate)
training = mydat[year(dat$date) < 2012,]
testing = mydat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
training = mydat[year(mydat$date) < 2012,]
testing = mydat[(year(mydat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
install.packages("forecast")
library(forecast)
?bats
bats(tstrain)
fit <- bats(tstrain)
plot(forecast(fit))
taylor.fit <- bats(taylor)
tstrain = ts(training$visitsTumblr)
tstest = ts(testing$visitsTumblr)
predict(fit, newdata=tstest, interval = 'confidence')
predict(fit, newdata=tstest, interval = 'prediction')
predict(fit, newdata=tstest)
forecast(fit, newdata=tstest)
forecast(fit, newdata=tstest)
forecast(fit, newdata=tstest)$Forecast
forecast(fit, newdata=tstest)$Point
forecast(fit, newdata=tstest)
forecast(fit, newdata=tstest)
forecast(fit, newdata=tstest)[0]
forecast(fit, newdata=tstest)[[0]]
?forecast
predict(fit, newdata=tstest)
forecast(fit)
fc = forecast(fit)
fc = forecast(fit)
fc.model
model(fc)
model.fc
fc.mdoel
fc.model
fc
fc$model
fc$x
fc$mean
tstest
training = mydat[year(mydat$date) < 2012,]
testing = mydat[(year(mydat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
tstest = ts(testing$visitsTumblr)
fit <- bats(tstrain)
fc = forecast(fit)
fc$x
fc$fitted
accuracy(fc, tstest)
tstest
accuracy(fc$lower, tstest)
fc$lower
fc$upper
tstest
tstest > 100
tstest < -299 OR tstest > 784
tstest < -299 | tstest > 784
accuracy_object <- tstest < -299 | tstest > 784
summary(accuracy_object)
(219+16)/219
219/(219+16)
set.seed(3523)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
install.packages("e1071")
library(e1071)
modFit <- train(CompressiveStrength ~ ., method = "e1071", data=training)
modFit <- train(CompressiveStrength ~ ., method = "svm_linear", data=training)
modFit <- train(CompressiveStrength ~ ., method = "svmlinear", data=training)
modFit <- train(CompressiveStrength ~ ., method = "svmLinear", data=training)
predict(modFit, newdata = testing)
RMSE(predict(modFit, newdata = testing), testing$CompressiveStrength)
set.seed(325)
modFit <- train(CompressiveStrength ~ ., method = "svmLinear", data=training)
RMSE(predict(modFit, newdata = testing), testing$CompressiveStrength)
set.seed(325)
svm.model <- svm(CompressiveStrength ~ ., data = training)
svm.pred <- predict(svm.model, testing$CompressiveStrength)
RMSE(svm.pred, testing$CompressiveStrength)
set.seed(325)
svm.model <- svm(CompressiveStrength ~ ., data = training)
svm.pred <- predict(svm.model, testing)
RMSE(svm.pred, testing$CompressiveStrength)
## Vraag 2
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
modFit_rf <- train(diagnosis ~ ., method = 'rf', data=training)
modFit_GBM <- train(diagnosis ~ ., method = 'gbm', data=training, verbose=FALSE)
modFit_lda <- train(diagnosis ~ ., method = 'lda', data=training)
pred_rf <- predict(modFit_rf, newdata=testing)
pred_GBM <- predict(modFit_GBM, newdata=testing)
pred_lda <- predict(modFit_lda, newdata=testing)
# Example of Stacking algorithms
# create submodels
control <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)
algorithmList <- c('rf', 'gbm', 'lda')
set.seed(62433)
models <- caretList(diagnosis~., data=training, trControl=control, methodList=algorithmList)
results <- resamples(models)
summary(results)
dotplot(results)
# stack using glm
stackControl <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)
set.seed(62433)
stack.glm <- caretStack(models, method="rf", metric="Accuracy", trControl=stackControl)
print(stack.glm)
pred_stack <- predict(stack.glm, newdata=testing)
confusionMatrix(pred_rf, testing$diagnosis)$overall['Accuracy']
confusionMatrix(pred_GBM, testing$diagnosis)$overall['Accuracy']
confusionMatrix(pred_lda, testing$diagnosis)$overall['Accuracy']
confusionMatrix(pred_stack, testing$diagnosis)$overall['Accuracy']
setwd("~/coursera_local/practical_machine_learning_course_assignment")
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5)
install.packages("knitr")
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5)
options(width=120)
library(lattice)
library(ggplot2)
library(plyr)
library(randomForest)
library(plyr)
library(dplyr)
training.raw <- read.csv("pml-training.csv")
training.raw <- read.csv("data/pml-training.csv")
training.raw <- read.csv("data/pml_training.csv")
testing.raw <- read.csv("data/pml_testing.csv")
View(training.raw)
View(training.raw)
dim(training.raw)
head(training.raw)
str(training.raw)
NAColumns <- which(colSums(is.na(training.raw) | training.raw=="") > nrow(training.raw) / 100 * 20)
training <- training.raw[,-NAColumns]
testing <- testing.raw[,-NAColumns]
testing <- testing[,-c(1, timeColumns )]
timeColumns <- grep("timestamp", names(training))
training <- training[,-c(1, timeColumns )]
testing <- testing[,-c(1, timeColumns )]
levels(training$classe)
depedentVarLevels <- levels(training$classe)
training <- data.frame(data.matrix(training))
training$classe <- factor(training$classe, labels=depedentVarLevels)
testing <- data.frame(data.matrix(testing))
set.seed(12345)
library(caret)
classeIndex <- which(names(training) == "classe")
partition <- createDataPartition(y=training$classe, p=0.75, list=FALSE)
training.t <- training[partition, ]
training.v <- training[-partition, ]
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5)
options(width=120)
library(lattice)
library(ggplot2)
library(plyr)
library(dplyr)
library(randomForest)
library(caret)
correlations <- cor(training.T[, -classeIndex], as.numeric(training.T$classe))
correlations <- cor(training.t[, -classeIndex], as.numeric(training.t$classe))
bestCorrelations <- subset(as.data.frame(as.table(correlations)), abs(Freq)>0.3)
bestCorrelations
correlations <- cor(training.t[, -classeIndex], as.numeric(training.t$classe))
bestCorrelations <- subset(as.data.frame(as.table(correlations)), abs(Freq)>0.25)
bestCorrelations
ibrary(Rmisc)
library(Rmisc)
install.packages("Rmisc")
library(Rmisc)
p1 <- ggplot(training.t, aes(classe,pitch_forearm)) +
geom_boxplot(aes(fill=classe))
p2 <- ggplot(training.t, aes(classe, magnet_arm_x)) +
geom_boxplot(aes(fill=classe))
multiplot(p1,p2,cols=2)
p1 <- ggplot(training.t, aes(classe,magnet_belt_y)) +
geom_boxplot(aes(fill=classe))
p2 <- ggplot(training.t, aes(classe, magnet_arm_x)) +
geom_boxplot(aes(fill=classe))
p3 <- ggplot(training.t, aes(classe,magnet_arm_y)) +
geom_boxplot(aes(fill=classe))
p4 <- ggplot(training.t, aes(classe, pitch_forearm)) +
geom_boxplot(aes(fill=classe))
multiplot(p1,p2,p3,p4,cols=4)
multiplot(p1,p2,p3,p4,cols=2)
install.packages("corrplot")
library(corrplot)
library(corrplot)
correlationMatrix <- cor(training.t[, -classeIndex])
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.9, exact=TRUE)
excludeColumns <- c(highlyCorrelated, classeIndex)
corrplot(correlationMatrix, method="color", type="lower", order="hclust", tl.cex=0.70, tl.col="black", tl.srt = 45, diag = FALSE)
pcaPreProcess.all <- preProcess(training.t[, -classeIndex], method = "pca", thresh = 0.99)
training.t.pca.all <- predict(pcaPreProcess.all, training.t[, -classeIndex])
training.t.pca.all <- predict(pcaPreProcess.all, training.t[, -classeIndex])
testing.pca.all <- predict(pcaPreProcess.all, testing[, -classeIndex])
pcaPreProcess.subset <- preProcess(training.t[, -excludeColumns], method = "pca", thresh = 0.99)
training.subSetTrain.pca.subset <- predict(pcaPreProcess.subset, training.t[, -excludeColumns])
training.subSetTest.pca.subset <- predict(pcaPreProcess.subset, training.t[, -excludeColumns])
testing.pca.subset <- predict(pcaPreProcess.subset, testing[, -classeIndex])
ntree <- 200
rfMod.cleaned <- randomForest(
x=training.subSetTrain[, -classeIndex],
y=training.subSetTrain$classe,
xtest=training.subSetTest[, -classeIndex],
ytest=training.subSetTest$classe,
ntree=ntree,
keep.forest=TRUE,
proximity=TRUE)
pcaPreProcess.all <- preProcess(training.t[, -classeIndex], method = "pca", thresh = 0.99)
training.t.pca.all <- predict(pcaPreProcess.all, training.t[, -classeIndex])
training.v.pca.all <- predict(pcaPreProcess.all, training.v[, -classeIndex])
testing.pca.all <- predict(pcaPreProcess.all, testing[, -classeIndex])
pcaPreProcess.subset <- preProcess(training.t[, -excludeColumns], method = "pca", thresh = 0.99)
training.t.pca.subset <- predict(pcaPreProcess.subset, t.subSetTrain[, -excludeColumns])
```{r}
pcaPreProcess.all <- preProcess(training.t[, -classeIndex], method = "pca", thresh = 0.99)
training.t.pca.all <- predict(pcaPreProcess.all, training.t[, -classeIndex])
training.v.pca.all <- predict(pcaPreProcess.all, training.v[, -classeIndex])
testing.pca.all <- predict(pcaPreProcess.all, testing[, -classeIndex])
pcaPreProcess.subset <- preProcess(training.t[, -excludeColumns], method = "pca", thresh = 0.99)
training.t.pca.subset <- predict(pcaPreProcess.subset, trainnig.t[, -excludeColumns])
pcaPreProcess.all <- preProcess(training.t[, -classeIndex], method = "pca", thresh = 0.99)
training.t.pca.all <- predict(pcaPreProcess.all, training.t[, -classeIndex])
training.v.pca.all <- predict(pcaPreProcess.all, training.v[, -classeIndex])
testing.pca.all <- predict(pcaPreProcess.all, testing[, -classeIndex])
pcaPreProcess.subset <- preProcess(training.t[, -excludeColumns], method = "pca", thresh = 0.99)
training.t.pca.subset <- predict(pcaPreProcess.subset, training.t[, -excludeColumns])
training.v.pca.subset <- predict(pcaPreProcess.subset, training.v[, -excludeColumns])
testing.pca.subset <- predict(pcaPreProcess.subset, testing[, -classeIndex])
ntree <- 200
rfMod.cleaned <- randomForest(
x=training.t[, -classeIndex],
y=training.t$classe,
xtest=training.v[, -classeIndex],
ytest=training.v$classe,
ntree=ntree,
keep.forest=TRUE,
proximity=TRUE)
rfMod.exclude <- randomForest(
x=training.t[, -excludeColumns],
y=training.t$classe,
xtest=training.v[, -excludeColumns],
ytest=training.v$classe,
ntree=ntree,
keep.forest=TRUE,
proximity=TRUE)
rfMod.pca.all <- randomForest(
x=training.t.pca.all,
y=training.t$classe,
xtest=training.v.pca.all,
ytest=training.v$classe,
ntree=ntree,
keep.forest=TRUE,
proximity=TRUE)
rfMod.pca.subset <- randomForest(
x=training.t.pca.subset,
y=training.t$classe,
xtest=training.v.pca.subset,
ytest=training.v$classe,
ntree=ntree,
keep.forest=TRUE,
proximity=TRUE)
rfMod.pca.subset <- randomForest(
x=training.t.pca.subset,
y=training.t$classe,
xtest=training.v.pca.subset,
ytest=training.v$classe,
ntree=ntree,
keep.forest=TRUE,
proximity=TRUE)
View(rfMod.cleaned)
accuracy(rfMod.cleaned)
rfMod.cleaned
rfMod.cleaned$err.rate
average(rfMod.cleaned$err.rate)
mean(rfMod.cleaned$err.rate)
paste0("Accuracy on training: ",1-mean(rfMod.cleaned$err.rate))
paste0("Accuracy on testing: ",1-mean(rfMod.cleaned$test$err.rate))
paste0("Accuracy on training: ",1-mean(rfMod.exclude$err.rate))
paste0("Accuracy on testing: ",1-mean(rfMod.exclude$test$err.rate))
paste0("Accuracy on training: ",1-mean(rfMod.exclude$err.rate))
paste0("Accuracy on testing: ",1-mean(rfMod.exclude$test$err.rate))
rfMod.exclude.training.acc <- round(1-sum(rfMod.exclude$confusion[, 'class.error']),3)
paste0("Accuracy on training: ",rfMod.exclude.training.acc)
rfMod.exclude.testing.acc <- round(1-sum(rfMod.exclude$test$confusion[, 'class.error']),3)
paste0("Accuracy on testing: ",rfMod.exclude.testing.acc)
rfMod.cleaned$test$confusion
rfMod.cleaned$test$err.rate
mean(rfMod.cleaned$test$err.rate)
1 - mean(rfMod.cleaned$test$err.rate)
paste0("Accuracy on training: ",1-mean(rfMod.pca.all$err.rate))
paste0("Accuracy on testing: ",1-mean(rfMod..pca.all$test$err.rate))
paste0("Accuracy on training: ",1-mean(rfMod.pca.all$err.rate))
paste0("Accuracy on testing: ",1-mean(rfMod.pca.all$test$err.rate))
paste0("Accuracy on training: ",1-mean(rfMod.cleaned$err.rate))
paste0("Accuracy on testing: ",1-mean(rfMod.cleaned$test$err.rate))
paste0("Accuracy on training: ",1-mean(rfMod.exclude$err.rate))
paste0("Accuracy on testing: ",1-mean(rfMod.exclude$test$err.rate))
paste0("Accuracy on training: ",1-mean(rfMod.pca.all$err.rate))
paste0("Accuracy on testing: ",1-mean(rfMod.pca.all$test$err.rate))
predict(rfMod.exclude, testing[, -excludeColumns])
rfMod.pca.all <- randomForest(
x=training.t.pca.all,
y=training.t$classe,
xtest=training.v.pca.all,
ytest=training.v$classe,
ntree=ntree,
keep.forest=TRUE,
proximity=TRUE)
training.raw <- read.csv("data/pml_training.csv")
testing.raw <- read.csv("data/pml_testing.csv")
NAColumns <- which(colSums(is.na(training.raw) | training.raw=="") > nrow(training.raw) / 100 * 20)
training <- training.raw[,-NAColumns]
testing <- testing.raw[,-NAColumns]
timeColumns <- grep("timestamp", names(training))
training <- training[,-c(1, timeColumns )]
testing <- testing[,-c(1, timeColumns )]
depedentVarLevels <- levels(training$classe)
training <- data.frame(data.matrix(training))
training$classe <- factor(training$classe, labels=depedentVarLevels)
testing <- data.frame(data.matrix(testing))
set.seed(12345)
classeIndex <- which(names(training) == "classe")
partition <- createDataPartition(y=training$classe, p=0.75, list=FALSE)
training.t <- training[partition, ]
training.v <- training[-partition, ]
correlations <- cor(training.t[, -classeIndex], as.numeric(training.t$classe))
bestCorrelations <- subset(as.data.frame(as.table(correlations)), abs(Freq)>0.25)
bestCorrelations
p1 <- ggplot(training.t, aes(classe,magnet_belt_y)) +
geom_boxplot(aes(fill=classe))
p2 <- ggplot(training.t, aes(classe, magnet_arm_x)) +
geom_boxplot(aes(fill=classe))
p3 <- ggplot(training.t, aes(classe,magnet_arm_y)) +
geom_boxplot(aes(fill=classe))
p4 <- ggplot(training.t, aes(classe, pitch_forearm)) +
geom_boxplot(aes(fill=classe))
multiplot(p1,p2,p3,p4,cols=2)
correlationMatrix <- cor(training.t[, -classeIndex])
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.9, exact=TRUE)
excludeColumns <- c(highlyCorrelated, classeIndex)
corrplot(correlationMatrix, method="color", type="lower", order="hclust", tl.cex=0.70, tl.col="black", tl.srt = 45, diag = FALSE)
pcaPreProcess.all <- preProcess(training.t[, -classeIndex], method = "pca", thresh = 0.99)
training.t.pca.all <- predict(pcaPreProcess.all, training.t[, -classeIndex])
training.v.pca.all <- predict(pcaPreProcess.all, training.v[, -classeIndex])
testing.pca.all <- predict(pcaPreProcess.all, testing[, -classeIndex])
pcaPreProcess.subset <- preProcess(training.t[, -excludeColumns], method = "pca", thresh = 0.99)
training.t.pca.subset <- predict(pcaPreProcess.subset, training.t[, -excludeColumns])
training.v.pca.subset <- predict(pcaPreProcess.subset, training.v[, -excludeColumns])
testing.pca.subset <- predict(pcaPreProcess.subset, testing[, -classeIndex])
rfMod.pca.subset <- randomForest(
x=training.t.pca.subset,
y=training.t$classe,
xtest=training.v.pca.subset,
ytest=training.v$classe,
ntree=ntree,
keep.forest=TRUE,
proximity=TRUE)
ntree <- 200
rfMod.cleaned <- randomForest(
x=training.t[, -classeIndex],
y=training.t$classe,
xtest=training.v[, -classeIndex],
ytest=training.v$classe,
ntree=ntree,
keep.forest=TRUE,
proximity=TRUE)
rfMod.exclude <- randomForest(
x=training.t[, -excludeColumns],
y=training.t$classe,
xtest=training.v[, -excludeColumns],
ytest=training.v$classe,
ntree=ntree,
keep.forest=TRUE,
proximity=TRUE)
rfMod.pca.all <- randomForest(
x=training.t.pca.all,
y=training.t$classe,
xtest=training.v.pca.all,
ytest=training.v$classe,
ntree=ntree,
keep.forest=TRUE,
proximity=TRUE)
rfMod.pca.subset <- randomForest(
x=training.t.pca.subset,
y=training.t$classe,
xtest=training.v.pca.subset,
ytest=training.v$classe,
ntree=ntree,
keep.forest=TRUE,
proximity=TRUE)
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5)
options(width=120)
library(lattice)
library(ggplot2)
library(plyr)
library(dplyr)
library(randomForest)
library(caret)
library(Rmisc)
library(ggplot2)
library(corrplot)
training.raw <- read.csv("data/pml_training.csv")
testing.raw <- read.csv("data/pml_testing.csv")
str(training.raw)
NAColumns <- which(colSums(is.na(training.raw) | training.raw=="") > nrow(training.raw) / 100 * 20)
training <- training.raw[,-NAColumns]
testing <- testing.raw[,-NAColumns]
timeColumns <- grep("timestamp", names(training))
training <- training[,-c(1, timeColumns )]
testing <- testing[,-c(1, timeColumns )]
depedentVarLevels <- levels(training$classe)
training <- data.frame(data.matrix(training))
training$classe <- factor(training$classe, labels=depedentVarLevels)
testing <- data.frame(data.matrix(testing))
set.seed(12345)
classeIndex <- which(names(training) == "classe")
partition <- createDataPartition(y=training$classe, p=0.75, list=FALSE)
training.t <- training[partition, ]
training.v <- training[-partition, ]
correlations <- cor(training.t[, -classeIndex], as.numeric(training.t$classe))
bestCorrelations <- subset(as.data.frame(as.table(correlations)), abs(Freq)>0.25)
bestCorrelations
p1 <- ggplot(training.t, aes(classe,magnet_belt_y)) +
geom_boxplot(aes(fill=classe))
p2 <- ggplot(training.t, aes(classe, magnet_arm_x)) +
geom_boxplot(aes(fill=classe))
p3 <- ggplot(training.t, aes(classe,magnet_arm_y)) +
geom_boxplot(aes(fill=classe))
p4 <- ggplot(training.t, aes(classe, pitch_forearm)) +
geom_boxplot(aes(fill=classe))
multiplot(p1,p2,p3,p4,cols=2)
correlationMatrix <- cor(training.t[, -classeIndex])
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.9, exact=TRUE)
excludeColumns <- c(highlyCorrelated, classeIndex)
corrplot(correlationMatrix, method="color", type="lower", order="hclust", tl.cex=0.70, tl.col="black", tl.srt = 45, diag = FALSE)
pcaPreProcess.all <- preProcess(training.t[, -classeIndex], method = "pca", thresh = 0.99)
training.t.pca.all <- predict(pcaPreProcess.all, training.t[, -classeIndex])
training.v.pca.all <- predict(pcaPreProcess.all, training.v[, -classeIndex])
testing.pca.all <- predict(pcaPreProcess.all, testing[, -classeIndex])
pcaPreProcess.subset <- preProcess(training.t[, -excludeColumns], method = "pca", thresh = 0.99)
training.t.pca.subset <- predict(pcaPreProcess.subset, training.t[, -excludeColumns])
training.v.pca.subset <- predict(pcaPreProcess.subset, training.v[, -excludeColumns])
testing.pca.subset <- predict(pcaPreProcess.subset, testing[, -classeIndex])
ntree <- 50
rfMod.cleaned <- randomForest(
x=training.t[, -classeIndex],
y=training.t$classe,
xtest=training.v[, -classeIndex],
ytest=training.v$classe,
ntree=ntree,
keep.forest=TRUE,
proximity=TRUE)
rfMod.exclude <- randomForest(
x=training.t[, -excludeColumns],
y=training.t$classe,
xtest=training.v[, -excludeColumns],
ytest=training.v$classe,
ntree=ntree,
keep.forest=TRUE,
proximity=TRUE)
rfMod.pca.all <- randomForest(
x=training.t.pca.all,
y=training.t$classe,
xtest=training.v.pca.all,
ytest=training.v$classe,
ntree=ntree,
keep.forest=TRUE,
proximity=TRUE)
rfMod.pca.subset <- randomForest(
x=training.t.pca.subset,
y=training.t$classe,
xtest=training.v.pca.subset,
ytest=training.v$classe,
ntree=ntree,
keep.forest=TRUE,
proximity=TRUE)
knit()
library(knitr)
knit()
knit()
predict(rfMod.exclude, testing[, -excludeColumns])
